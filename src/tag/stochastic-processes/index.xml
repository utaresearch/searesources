<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stochastic Processes | Erick Jones</title>
    <link>/tag/stochastic-processes/</link>
      <atom:link href="/tag/stochastic-processes/index.xml" rel="self" type="application/rss+xml" />
    <description>Stochastic Processes</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>&amp;copy Erick Jones {2020}</copyright><lastBuildDate>Sat, 04 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Stochastic Processes</title>
      <link>/tag/stochastic-processes/</link>
    </image>
    
    <item>
      <title>Basics of Markov Chains</title>
      <link>/post/orie/markovchains/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/orie/markovchains/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post explores how to markov chains work and how to visulaize them in R.
I use a R package specifically designed to visualize markov chains.
I also represent these markov chains using tables.
This is a reproducible example if you have R Studio just make sure you have installed the correct packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(markovchain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;markovchain&amp;#39; was built under R version 4.0.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(diagram)
#Allows the use of exponential operators in matrix
library(expm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;expm&amp;#39; was built under R version 4.0.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(matlib)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A good article about Markov Chain Monte Carlo Methods https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50

# A Simple Example from https://www.analyticsvidhya.com/blog/2014/07/markov-chain-simplified/
# Creating a transition matrix
trans_mat &amp;lt;- matrix(c(0.7,0.3,0.1,0.9),nrow = 2, byrow = TRUE)
trans_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]  0.7  0.3
## [2,]  0.1  0.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create the Discrete Time Markov Chain
disc_trans &amp;lt;- new(&amp;quot;markovchain&amp;quot;,transitionMatrix=trans_mat, states=c(&amp;quot;Pepsi&amp;quot;,&amp;quot;Coke&amp;quot;), name=&amp;quot;MC 1&amp;quot;) 
mcDF &amp;lt;- as(disc_trans,&amp;quot;data.frame&amp;quot;)
mcDF&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      t0    t1 prob
## 1 Pepsi Pepsi  0.7
## 2 Pepsi  Coke  0.3
## 3  Coke Pepsi  0.1
## 4  Coke  Coke  0.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disc_trans&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MC 1 
##  A  2 - dimensional discrete Markov Chain defined by the following states: 
##  Pepsi, Coke 
##  The transition matrix  (by rows)  is defined as follows: 
##       Pepsi Coke
## Pepsi   0.7  0.3
## Coke    0.1  0.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ORIE/markovchains_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Market Share after one month
Current_state &amp;lt;- c(0.55,0.45)
steps &amp;lt;- 1
finalState &amp;lt;- Current_state*disc_trans^steps #using power operator
finalState&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Pepsi Coke
## [1,]  0.43 0.57&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Market Share after two month
Current_state &amp;lt;- c(0.55,0.45)
steps &amp;lt;- 2
finalState &amp;lt;- Current_state*disc_trans^steps #using power operator
finalState&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Pepsi  Coke
## [1,] 0.358 0.642&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Markov Chain Statistical Operations
steadyStates(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Pepsi Coke
## [1,]  0.25 0.75&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanFirstPassageTime(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Pepsi     Coke
## Pepsi     0 3.333333
## Coke     10 0.000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanRecurrenceTime(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Pepsi     Coke 
## 4.000000 1.333333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hittingProbabilities(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Pepsi Coke
## Pepsi     1    1
## Coke      1    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanAbsorptionTime(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## named numeric(0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#absorptionProbabilities(disc_trans)
period(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MC 1  Markov chain that is composed by: 
## Closed classes: 
## Pepsi Coke 
## Recurrent classes: 
## {Pepsi,Coke}
## Transient classes: 
## NONE 
## The Markov chain is irreducible 
## The absorbing states are: NONE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Manually Calculating Markov Chains
#https://www.probabilitycourse.com/chapter11/11_2_1_introduction.php

#Chapman-Kolmogorov Equation P^(n) = P^n
#p_ij^(m+n) = P(X_m+n = j | X_0 = i) = sum(p_ik^(m)*p_kj^(n))

#Probabilty Space after 5 steps
steps &amp;lt;- 5

Current_state%*%(trans_mat%^%steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]
## [1,] 0.273328 0.726672&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Mean Return and Mean Hitting Times using Recursive Equations
#r_l = 1 + sum(t_k*p_lk)
#t_l = 0; t_k = 1 + sum(t_j*p_kj)

#Given X_0 = Coke time until pepsi first time, t_pepsi = 0
# t_coke = 1 + 1/10*t_pepsi + 9/10t_coke
t_coke &amp;lt;- solve(1/10,1)
#r_pepsi = 1 + 7/10*t_pepsi + 3/10*t_coke
r_pepsi &amp;lt;- 1 + 3/10*t_coke

meanFirstPassageTime(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Pepsi     Coke
## Pepsi     0 3.333333
## Coke     10 0.000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t_coke&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanRecurrenceTime(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Pepsi     Coke 
## 4.000000 1.333333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_pepsi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Steady State
#Stationary Distribtution pi = pi*P, sum(pi) = 1 and if irreducible and aperiodic pi_j = lim(n&amp;gt;inf)P(X_n =j | X_0 = i)
#pi_p = 7/10pi_p+1/10pi_c; pi_c = 3/10pi_p + 9/10pi_c, pi_c+pi_p =1
A &amp;lt;- matrix(c(-3/10,1/10,3/10,-1/10,1,1), nrow =3, byrow = TRUE )
B &amp;lt;- c(0,0,1)
steadyStates(disc_trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Pepsi Coke
## [1,]  0.25 0.75&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Solve(A,B)




#rm(Current_state, disc_trans, finalState,steps,trans_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Continous Time Markov Chains

energyStates &amp;lt;- c(&amp;quot;sigma&amp;quot;, &amp;quot;sigma_star&amp;quot;)
#Must produce generator matrix from a transistion probablity matrix
Q &amp;lt;- expm::logm(disc_trans@transitionMatrix,method=&amp;#39;Eigen&amp;#39;)

gen &amp;lt;- matrix(data = c(-3, 3, 1, -1), nrow = 2, byrow = TRUE, dimnames = list(energyStates, energyStates))

molecularCTMC &amp;lt;- new(&amp;quot;ctmc&amp;quot;, states = energyStates, byrow = TRUE, generator = gen, name = &amp;quot;Molecular Transition Model&amp;quot;)

statesDist &amp;lt;- c(0.8, 0.2)
rctmc(n = 3, ctmc = molecularCTMC, initDist = statesDist, out.type = &amp;quot;df&amp;quot;, include.T0 = FALSE, T = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       states              time
## 1      sigma 0.490779113024473
## 2 sigma_star 0.893907884742721
## 3      sigma  1.83824493102602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steadyStates(molecularCTMC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      sigma sigma_star
## [1,]  0.25       0.75&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Q-Learning with Liars Dice
#http://gradientdescending.com/q-learning-example-with-liars-dice-in-r/

# play a round of liars dice
liars.dice.round &amp;lt;- function(players, control, player.dice.count, agents, game.states, reward, Q.mat, a = 1, verbose = 1){
  
  # set array for recording results
  y.ctrl = c(); y.state = c(); y.action = c()
  
  # roll the dice for each player
  if(verbose &amp;gt; 0) cat(&amp;quot;\n\n&amp;quot;)
  rolls &amp;lt;- lapply(1:players, function(x) sort(sample(1:6, player.dice.count[[x]], replace = TRUE)))
  if(verbose &amp;gt; 1) lapply(rolls, function(x) cat(&amp;quot;dice: &amp;quot;, x, &amp;quot;\n&amp;quot;))
  total.dice &amp;lt;- sum(unlist(player.dice.count))
  
  # set penalty
  penalty &amp;lt;- sapply(1:players, function(x) 0, simplify = FALSE)
  
  # print dice blocks
  if(verbose &amp;gt; 0) Dice(rolls[[1]])
  
  # set up roll table
  roll.table &amp;lt;- roll.table.fn(rolls)
  
  # initial bid
  if(verbose &amp;gt; 0) cat(&amp;quot;place first bid\nPlayer&amp;quot;, control, &amp;quot;has control\n&amp;quot;)
  if(control == a){
    
    dice.value &amp;lt;- set.dice.value(&amp;quot;dice value: &amp;quot;, 6)
    dice.quantity &amp;lt;- set.dice.value(&amp;quot;quantity; &amp;quot;, sum(roll.table))
    
  }else{
    
    # agent plays
    p1.state &amp;lt;- which(game.states$total == total.dice &amp;amp; game.states$p1 == player.dice.count[[1]] &amp;amp; game.states$prob_cat == total.dice)
    pars &amp;lt;- list(dice = rolls[[control]], total.dice = total.dice, dice.value = NULL, dice.quantity = 0, p1.state = p1.state)
    agent.action &amp;lt;- agents[[control]](pars = pars, Q.mat = Q.mat)
    dice.value &amp;lt;- agent.action$dice.value
    dice.quantity &amp;lt;- agent.action$dice.quantity
    
  }
  
  
  # calculate probability cat and determine the game state
  # action set to raise because you can&amp;#39;t call without an initial bid
  # this could be a 3rd action (initial bid) but it&amp;#39;s not really necessary
  player.dice.qty &amp;lt;- table(rolls[[1]])[as.character(dice.value)]
  player.dice.qty &amp;lt;- ifelse(is.na(player.dice.qty), 0, player.dice.qty) %&amp;gt;% unname
  prob.cat &amp;lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
  p1.state &amp;lt;- which(game.states$total == total.dice &amp;amp; game.states$p1 == player.dice.count[[1]] &amp;amp; game.states$prob_cat == prob.cat)
  p1.action &amp;lt;- &amp;quot;raise&amp;quot;
  
  # storing states for Q iteration
  y.ctrl = c(); y.state = c(); y.action = c()
  
  # moving control to the next player
  # storing the previous player since if the next player calls the previous player could lose a die
  prev &amp;lt;- control
  control &amp;lt;- control %% players + 1
  if(verbose &amp;gt; 0) cat(&amp;quot;dice value &amp;quot;, dice.value, &amp;quot;; dice quantity &amp;quot;, dice.quantity, &amp;quot;\n&amp;quot;)
  
  
  # loop through each player and continue until there is a winner and loser
  called &amp;lt;- FALSE
  while(!called){
    
    # check if the player with control is still in the game - if not skip
    if(player.dice.count[[control]] &amp;gt; 0){
      if(control == a){
        
        action &amp;lt;- readline(&amp;quot;raise or call (r/c)? &amp;quot;)
        
      }else{
        
        # the agent makes a decision
        pars &amp;lt;- list(dice = rolls[[control]], total.dice = total.dice, dice.value = dice.value, dice.quantity = dice.quantity, p1.state = p1.state)
        agent.action &amp;lt;- agents[[control]](pars = pars, Q.mat = Q.mat)
        action &amp;lt;- agent.action$action
        
      }
      
      
      # storing states for reward iteration
      if(control == 1 &amp;amp; !is.null(agent.action$action)){
        player.dice.qty &amp;lt;- table(rolls[[1]])[as.character(dice.value)]
        player.dice.qty &amp;lt;- ifelse(is.na(player.dice.qty), 0, player.dice.qty) %&amp;gt;% unname
        
        p1.action &amp;lt;- agent.action$action
        prob.cat &amp;lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
        p1.state &amp;lt;- which(game.states$total == total.dice &amp;amp; game.states$p1 == player.dice.count[[1]] &amp;amp; game.states$prob_cat == prob.cat)
      }
      
      
      # called
      if(action %in% c(&amp;quot;call&amp;quot;, &amp;quot;c&amp;quot;)){
        
        if(verbose &amp;gt; 0) {
          cat(&amp;quot;player&amp;quot;, control, &amp;quot;called\nRoll table\n&amp;quot;)
          print(roll.table)
        }
        
        # dice are reavealed
        
        # check if the quantity of dice value is less or more than the total in the pool
        # if more control loses otherwise control-1 win
        if(dice.quantity &amp;gt; roll.table[dice.value]){
          
          penalty[[prev]] &amp;lt;- penalty[[prev]] - 1
          if(verbose &amp;gt; 0) cat(&amp;quot;player&amp;quot;, prev, &amp;quot;lost a die\n&amp;quot;)
          
        }else{
          
          penalty[[control]] &amp;lt;- penalty[[control]] - 1
          if(verbose &amp;gt; 0) cat(&amp;quot;player&amp;quot;, control, &amp;quot;lost a die\n&amp;quot;)
          
        }
        
        # for Q iteration
        y.ctrl &amp;lt;- c(y.ctrl, control); y.state &amp;lt;- c(y.state, p1.state); y.action &amp;lt;- c(y.action, p1.action)
        
        # if called use the penalty array to change states
        prob.cat &amp;lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
        p1.state &amp;lt;- which(game.states$total == total.dice-1 &amp;amp; game.states$p1 == player.dice.count[[1]]+penalty[[1]] &amp;amp; game.states$prob_cat == prob.cat)
        
        # break the loop
        called &amp;lt;- TRUE
        
      }else{
        
        if(verbose &amp;gt; 0) cat(&amp;quot;player&amp;quot;, control, &amp;quot;raised\n&amp;quot;)
        
        if(control == a){
          
          # player sets next dice value
          dice.value &amp;lt;- set.dice.value(&amp;quot;dice value: &amp;quot;, 6)
          dice.quantity &amp;lt;- set.dice.value(&amp;quot;quantity; &amp;quot;, sum(roll.table))
          
        }else{
          
          dice.value &amp;lt;- agent.action$dice.value
          dice.quantity &amp;lt;- agent.action$dice.quantity
        }
        
        # p1 state after the raise
        prob.cat &amp;lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
        p1.state &amp;lt;- which(game.states$total == total.dice &amp;amp; game.states$p1 == player.dice.count[[1]] &amp;amp; game.states$prob_cat == prob.cat)
        if(verbose &amp;gt; 0) cat(&amp;quot;dice value&amp;quot;, dice.value, &amp;quot;; dice quantity&amp;quot;, dice.quantity, &amp;quot;\n&amp;quot;)
      }
      
      # store info for Q update
      y.ctrl &amp;lt;- c(y.ctrl, control); y.state &amp;lt;- c(y.state, p1.state); y.action &amp;lt;- c(y.action, p1.action)
      
      # set the control player to now be the previous player
      prev &amp;lt;- control
    }
    
    # next player has control
    control &amp;lt;- control %% players + 1
  }
  
  # play results and return
  play &amp;lt;- data.frame(y.ctrl, y.state, y.action)
  return(list(penalty = penalty, play = play))
}








# play a full game of liars dice
play.liars.dice &amp;lt;- function(players = 4, num.dice = 6, auto = FALSE, verbose = 1, agents, Q.mat = NULL, train = FALSE, print.trans = FALSE){
  
  # begin!
  if(verbose &amp;gt; 0) liars.dice.title()
  
  # setting the number of dice each player has
  ndice &amp;lt;- sapply(rep(num.dice, players), function(x) x, simplify = FALSE)
  players.left &amp;lt;- sum(unlist(ndice) &amp;gt; 0)
  
  # setting game states matrix
  game.states &amp;lt;- generate.game.states(players, num.dice)
  
  # set up reward matrix
  reward &amp;lt;- generate.reward.matrix(game.states)
  reward &amp;lt;- list(raise = reward, call = reward)
  
  # set Q matrix if null
  if(is.null(Q.mat)) Q.mat &amp;lt;- matrix(0, nrow = nrow(reward$raise), ncol = length(reward), dimnames = list(c(), names(reward)))
  
  # while there is at least 2 left in the game
  # who has control
  ctrl &amp;lt;- sample(1:players, 1)
  play.df &amp;lt;- data.frame()
  while(players.left &amp;gt; 1){
    
    # play a round
    results &amp;lt;- liars.dice.round(
      players = players, 
      control = ctrl,
      player.dice.count = ndice, 
      game.states = game.states,
      reward = reward,
      Q.mat = Q.mat,
      agents = agents,
      a = as.numeric(!auto),
      verbose = verbose
    )
    
    # update how many dice the players are left with given the 
    # outcomes of the round
    for(k in seq_along(ndice)){
      ndice[[k]] &amp;lt;- ndice[[k]] + results$penalty[[k]]
      if(ndice[[k]] == 0 &amp;amp; results$penalty[[k]] == -1){
        if(verbose &amp;gt; 0) cat(&amp;quot;player&amp;quot;, k, &amp;quot;is out of the game\n&amp;quot;)
      }
      
      # update who has control so they can start the bidding
      if(results$penalty[[k]] == -1){
        ctrl &amp;lt;- k
        while(ndice[[ctrl]] == 0){
          ctrl &amp;lt;- ctrl %% players + 1
        }
      }
    }
    
    # checking how many are left and if anyone won the game
    players.left &amp;lt;- sum(unlist(ndice) &amp;gt; 0)
    if(players.left == 1){
      if(verbose &amp;gt; 0) cat(&amp;quot;player&amp;quot;, which(unlist(ndice) &amp;gt; 0), &amp;quot;won the game\n&amp;quot;)
    }
    
    # appending play
    play.df &amp;lt;- rbind(play.df, results$play)
  }
  
  if(print.trans) print(play.df)
  
  # update Q
  # rather than training after each action, training at the 
  # end of each game in bulk
  # just easier this way
  if(train) Q.mat &amp;lt;- update.Q(play.df, Q.mat, reward)
  
  # return the winner and Q matrix
  return(list(winner = which(unlist(ndice) &amp;gt; 0), Q.mat = Q.mat))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Other Stochastic Processes
#Martingales http://gradientdescending.com/martingale-strategies-dont-work-but-we-knew-that-simulation-analysis-in-r/
#https://github.com/doehm/martingale

#Bayesian Networks http://gradientdescending.com/simulating-data-with-bayesian-networks/

#Other Q Learning https://www.r-bloggers.com/a-simple-intro-to-q-learning-in-r-floor-plan-navigation/
#https://dataaspirant.com/2018/02/05/reinforcement-learning-r/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add a new chunk by clicking the &lt;em&gt;Insert Chunk&lt;/em&gt; button on the toolbar or by pressing &lt;em&gt;Ctrl+Alt+I&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the &lt;em&gt;Preview&lt;/em&gt; button or press &lt;em&gt;Ctrl+Shift+K&lt;/em&gt; to preview the HTML file).&lt;/p&gt;
&lt;p&gt;The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike &lt;em&gt;Knit&lt;/em&gt;, &lt;em&gt;Preview&lt;/em&gt; does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
